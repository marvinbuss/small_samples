{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Azure ML Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(f\"SDK version: {azureml.core.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Authenticate and initielize Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\n",
    "    f\"Workspace name: {ws.name}\", \n",
    "    f\"Azure region: {ws.location}\", \n",
    "    f\"Subscription id: {ws.subscription_id}\", \n",
    "    f\"Resource group: {ws.resource_group}\",\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "cluster_name = os.environ.get(\"AML_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "cluster_sku = os.environ.get(\"AML_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "cluster_priority = os.environ.get(\"AML_CLUSTER_SKU\", \"dedicated\")\n",
    "cluster_min_nodes = os.environ.get(\"AML_CLUSTER_MIN_NODES\", 0)\n",
    "cluster_max_nodes = os.environ.get(\"AML_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "try:\n",
    "    compute = AmlCompute(\n",
    "        workspace=ws,\n",
    "        name=cluster_name\n",
    "    )\n",
    "    print(\"Loaded existing aml cluster\")\n",
    "except ComputeTargetException as exception:\n",
    "    print(f\"Could not load aml cluster: {exception}\")\n",
    "    print(\"Creating new aml cluster\")\n",
    "    aml_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=cluster_sku,\n",
    "        vm_priority=cluster_priority,\n",
    "        min_nodes=0,\n",
    "        max_nodes=4,\n",
    "        idle_seconds_before_scaledown=300\n",
    "    )\n",
    "    compute = AmlCompute.create(\n",
    "        workspace=ws,\n",
    "        name=cluster_name,\n",
    "        provisioning_configuration=aml_config\n",
    "    )\n",
    "    \n",
    "    compute.wait_for_completion(\n",
    "        show_output=True\n",
    "    )\n",
    "\n",
    "print(compute.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Upload and register data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "print(\n",
    "    f\"Datastore name: {datastore.name}\",\n",
    "    f\"Datastore type: {datastore.datastore_type}\",\n",
    "    f\"Datastore account name: {datastore.account_name}\",\n",
    "    f\"Datastore container name: {datastore.container_name}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset\n",
    "datastore.upload_files(\n",
    "    files=[\"./train_dataset/iris.csv\"],\n",
    "    target_path=\"train_dataset/iris.csv\",\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Register as file dataset\n",
    "file_dataset = Dataset.File.from_files(\n",
    "    path=[(datastore, \"train_dataset/iris.csv\")]\n",
    ")\n",
    "file_dataset = file_dataset.register(\n",
    "    workspace=ws,\n",
    "    name=\"iris_file\",\n",
    "    description=\"iris file dataset\",\n",
    "    create_new_version=True\n",
    ")\n",
    "file_dataset.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_folder = \"steps\"\n",
    "script_file_name = \"preprocess.py\"\n",
    "script_file_name_dataset = \"preprocess_dataset.py\"\n",
    "\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/$script_file_name\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from azureml.core import Run\n",
    "run = Run.get_context()\n",
    "\n",
    "def main(args):\n",
    "    print(f\"Args: {args}\")\n",
    "    \n",
    "    # Load data paths\n",
    "    print(f\"Input: {args.input_path}\")\n",
    "    print(f\"Output: {args.output_path}\")\n",
    "    \n",
    "    # Get file paths\n",
    "    input_file_paths = get_file_list(args.input_path)\n",
    "    print(f\"Input file paths: {input_file_paths}\")\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(args.output_path, exist_ok=True)\n",
    "    \n",
    "    # Load input data\n",
    "    for i, input_file_path in enumerate(input_file_paths):\n",
    "        df = pd.read_csv(input_file_path, engine='python')\n",
    "        print(df)\n",
    "        df.to_csv(os.path.join(args.output_path, f\"myfile{i}.csv\"))\n",
    "\n",
    "\n",
    "def get_file_list(path):\n",
    "    path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root, filename)\n",
    "            path_list.append(path)\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Argument Parser Sample\")\n",
    "    parser.add_argument(\"--input_path\", type=str, help=\"argument sample\")\n",
    "    parser.add_argument(\"--output_path\", type=str, help=\"argument sample\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/$script_file_name_dataset\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from azureml.core import Run\n",
    "run = Run.get_context()\n",
    "\n",
    "def main(args):\n",
    "    print(f\"Args: {args}\")\n",
    "    \n",
    "    # Load data paths\n",
    "    print(f\"Dataset name: {args.dataset_name}\")\n",
    "    print(f\"Output: {args.output_path}\")\n",
    "    \n",
    "    # Load dataset path if you use datasets\n",
    "    input_path = run.input_datasets[args.dataset_name]\n",
    "    \n",
    "    # Get file paths\n",
    "    input_file_paths = get_file_list(input_path)\n",
    "    print(f\"Input file paths: {input_file_paths}\")\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(args.output_path, exist_ok=True)\n",
    "    \n",
    "    # Load input data\n",
    "    for i, input_file_path in enumerate(input_file_paths):\n",
    "        df = pd.read_csv(input_file_path, engine='python')\n",
    "        print(df)\n",
    "        df.to_csv(os.path.join(args.output_path, f\"myfile{i}.csv\"))\n",
    "\n",
    "\n",
    "def get_file_list(path):\n",
    "    path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root, filename)\n",
    "            path_list.append(path)\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Argument Parser Sample\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, help=\"argument sample\")\n",
    "    parser.add_argument(\"--output_path\", type=str, help=\"argument sample\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.module import Module\n",
    "from azureml.pipeline.core.graph import InputPortDef, OutputPortDef\n",
    "\n",
    "module_name = os.environ.get(\"MODULE_NAME\", \"mystep\")\n",
    "\n",
    "input_def = InputPortDef(\n",
    "    name=\"input\",\n",
    "    default_data_reference_name=datastore.name,\n",
    "    default_datastore_mode=\"mount\",\n",
    "    label=\"input\"\n",
    ")\n",
    "output_def = OutputPortDef(\n",
    "    name=\"output\",\n",
    "    default_datastore_name=datastore.name,\n",
    "    default_datastore_mode=\"mount\",\n",
    "    label=\"output\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    module = Module.create(\n",
    "        workspace=ws,\n",
    "        name=module_name,\n",
    "        description=\"A sample module.\"\n",
    "    )\n",
    "    module_version = module.publish_python_script(\n",
    "        script_name=script_file_name,\n",
    "        source_directory=script_folder,\n",
    "        description=\"Sample module\",\n",
    "        version=\"1\",\n",
    "        inputs=[input_def],\n",
    "        outputs=[output_def],\n",
    "        is_default=True\n",
    "    )\n",
    "except:\n",
    "    module = Module.get(\n",
    "        workspace=ws,\n",
    "        name=module_name\n",
    "    )\n",
    "    \n",
    "    module_version = module.publish_python_script(\n",
    "        script_name=script_file_name,\n",
    "        source_directory=script_folder,\n",
    "        description=\"Sample module\",\n",
    "        version=\"2\",\n",
    "        inputs=[input_def],\n",
    "        outputs=[output_def],\n",
    "        is_default=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import RunConfiguration\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "# Create conda dependencies\n",
    "dependencies = CondaDependencies.create(\n",
    "    pip_packages=[\"azureml-dataprep[pandas,fuse]\", \"azureml-defaults\", \"pandas\"],\n",
    "    conda_packages=[],\n",
    "    python_version=\"3.6.2\"\n",
    ")\n",
    "\n",
    "# Create run configuration\n",
    "run_config = RunConfiguration(\n",
    "    conda_dependencies=dependencies,\n",
    "    framework=\"Python\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, PipelineParameter\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "\n",
    "# Create PipelineParameter for dynamic pipeline input\n",
    "input_path = DataPath(\n",
    "    datastore=datastore,\n",
    "    path_on_datastore=\"train_dataset/iris.csv\"\n",
    ")\n",
    "input_path_pipeline_parameter = PipelineParameter(\n",
    "    name=\"input_path\",\n",
    "    default_value=input_path\n",
    ")\n",
    "input_data = (input_path_pipeline_parameter, DataPathComputeBinding(mode=\"mount\"))\n",
    "\n",
    "# OPTION 2\n",
    "# Create DataReference for static input\n",
    "#from azureml.data.data_reference import DataReference\n",
    "#input_data = DataReference(\n",
    "#    datastore=datastore,\n",
    "#    data_reference_name=\"iris\",\n",
    "#    path_on_datastore=\"train_dataset/iris.csv\",\n",
    "#    mode=\"mount\"\n",
    "#)\n",
    "\n",
    "# OPTION 3\n",
    "# Use dataset as input\n",
    "#input_dataset_name = \"input_path\"\n",
    "#input_data = file_dataset.as_named_input(input_dataset_name).as_mount()\n",
    "\n",
    "# Create PipelineData for output\n",
    "output_data = PipelineData(\n",
    "    name=\"output\",\n",
    "    datastore=datastore,\n",
    "    output_mode=\"mount\"\n",
    ")\n",
    "\n",
    "# Create wiring\n",
    "input_wiring = {\"input\": input_data}\n",
    "output_wiring = {\"output\": output_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ModuleStep\n",
    "\n",
    "step = ModuleStep(\n",
    "    module_version=module_version,\n",
    "    inputs_map=input_wiring,\n",
    "    outputs_map=output_wiring,\n",
    "    runconfig=run_config,\n",
    "    compute_target=compute,\n",
    "    arguments=[\"--input_path\", input_data,\n",
    "               \"--output_path\", output_data],\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Create a PythonScriptStep\n",
    "#step = PythonScriptStep(\n",
    "#    name=module_name,\n",
    "#    script_name=script_file_name,\n",
    "#    source_directory=script_folder,\n",
    "#    arguments=[\"--input_path\", input_data,\n",
    "#               \"--output_path\", output_data],\n",
    "#    compute_target=compute,\n",
    "#    runconfig=run_config,\n",
    "#    inputs=[input_data],\n",
    "#    outputs=[output_data],\n",
    "#    allow_reuse=True,\n",
    "#    version=\"1\"\n",
    "#)\n",
    "\n",
    "# Create a PythonScriptStep with datasets\n",
    "#step = PythonScriptStep(\n",
    "#    name=module_name,\n",
    "#    script_name=script_file_name_dataset,\n",
    "#    source_directory=script_folder,\n",
    "#    arguments=[\"--dataset_name\", input_dataset_name,\n",
    "#               \"--output_path\", output_data],\n",
    "#    compute_target=compute,\n",
    "#    runconfig=run_config,\n",
    "#    inputs=[input_data],\n",
    "#    outputs=[output_data],\n",
    "#    allow_reuse=True,\n",
    "#    version=\"1\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    workspace=ws,\n",
    "    steps=[step]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = os.environ.get(\"EXPERIMENT_NAME\", \"modulesample\")\n",
    "\n",
    "experiment  = Experiment(\n",
    "    workspace=ws,\n",
    "    name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run = experiment.submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Get Output path via Azure ML SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "module_step_run = run.find_step_run(name=module_name)[0]\n",
    "module_step_run_id = module_step_run.id\n",
    "module_step_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_step_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "module_step_run.get_output_data(name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get outputs of steps\n",
    "for step in run.get_steps():\n",
    "    print(f\"Output of step {step.name}\")\n",
    "    \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        output_ref = output.get_port_data_reference()\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Datastore: {output_ref.datastore_name}\")\n",
    "        print(f\"Path on Datastore: {output_ref.path_on_datastore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Publish Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = run.publish_pipeline(\n",
    "    name=\"SamplePipeline\",\n",
    "    description=\"My sample pipeline\",\n",
    "    continue_on_step_failure=True,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Use REST endpoint to submit a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "# Authentication\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "# Endpoint URI\n",
    "endpoint = published_pipeline.endpoint\n",
    "\n",
    "# Request with HTTP request\n",
    "response = requests.post(\n",
    "    endpoint,\n",
    "    headers=aad_token,\n",
    "    json={\n",
    "        \"ExperimentName\": experiment_name,\n",
    "        \"RunSource\": \"SDK\",\n",
    "        \"ParameterAssignments\": {},\n",
    "        \"DataPathAssignments\": {\n",
    "            \"input_path\": {\n",
    "                \"DataStoreName\": datastore.name,\n",
    "                \"RelativePath\": \"train_dataset/iris.csv\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response.raise_for_status()\n",
    "except Exception:    \n",
    "    raise Exception('Received bad response from the endpoint: {}\\n'\n",
    "                    'Response Code: {}\\n'\n",
    "                    'Headers: {}\\n'\n",
    "                    'Content: {}'.format(rest_endpoint, response.status_code, response.headers, response.content))\n",
    "\n",
    "run_id = response.json().get('Id')\n",
    "print(f\"Submitted pipeline run: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "import time, json\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "\n",
    "hosturl = f\"https://{ws.location}.api.azureml.ms/\"\n",
    "history_base = \"history/v1.0/\"\n",
    "resource_base = f\"subscriptions/{ws.subscription_id}/resourceGroups/{ws.resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws.name}/\"\n",
    "rundetails_base = f\"experiments/{experiment_name}/runs/{run_id}/details\"\n",
    "endpoint_rundetails = hosturl + history_base + resource_base + rundetails_base\n",
    "\n",
    "print(\"Waiting for run to be completed\")\n",
    "while True:\n",
    "    response_rundetails = requests.get(\n",
    "        endpoint_rundetails,\n",
    "        headers=auth.get_authentication_header()\n",
    "    )\n",
    "    status = json.loads(response_rundetails.content)[\"status\"]\n",
    "    print(f\"Current status: {status}\")\n",
    "    if status == \"Completed\":\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Get Outputh Path via REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_base = f\"experiments/{experiment_name}/runs/{run_id}/children\"\n",
    "endpoint_children = hosturl + history_base + resource_base + children_base\n",
    "\n",
    "response_children = requests.get(\n",
    "    endpoint_children,\n",
    "    headers=auth.get_authentication_header()\n",
    ")\n",
    "step_list = json.loads(response_children.content)[\"value\"]\n",
    "for step in step_list:\n",
    "    if step[\"name\"] == module_name:\n",
    "        step_id = step[\"runId\"]\n",
    "        break\n",
    "\n",
    "print(f\"Id of step: {step_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "\n",
    "hosturl = f\"https://{ws.location}.api.azureml.ms/\"\n",
    "history_base = \"history/v1.0/\"\n",
    "resource_base = f\"subscriptions/{ws.subscription_id}/resourceGroups/{ws.resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws.name}/\"\n",
    "rundetails_base = f\"experiments/{experiment_name}/runs/{step_id}/details\"\n",
    "endpoint_rundetails = hosturl + history_base + resource_base + rundetails_base\n",
    "\n",
    "response_rundetails = requests.get(\n",
    "    endpoint_rundetails,\n",
    "    headers=auth.get_authentication_header()\n",
    ")\n",
    "\n",
    "output_details = json.loads(response_rundetails.content)[\"runDefinition\"][\"dataReferences\"][\"output\"]\n",
    "print(f\"Output details: {output_details}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
